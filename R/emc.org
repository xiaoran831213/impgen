#+AUTHOR: Tong, Xiaoran
#+TITLE: Covariance of Incomplete Data by EM
#+PROPERTY: eval:no
#+OPTIONS: ^{}

Suppose $X$ contains N samples  drawn from M-dimensional multivariate normal of
mean $\mu$ and covariance $\Sigma$, that is,
\begin{equation}
x_{i.} \sim \mathcal{N}(
       \boldsymbol{\mu}, \boldsymbol{\Sigma}), \quad i=1 \dots N
\end{equation}

* Inital Estimate
The naive  estimates of $\mu_j$ and  $\sigma_{jk}$ can be the  mean of complete
samples in $x_{.j}$, and the covariance  between complete pairs of $x_{.j}$ and
$x_{.k}$, respectively.

\begin{equation}
	\mu_j^0 = \frac{\sum_i\delta_{ij} x_{ij}}{\sum_i{\delta_{ij}}},
	\quad j = 1 \dots M
\end{equation}
where $\delta_{ij} = 1$ indicates that $x_{ij}$ is observed, and 0 otherwise.

\begin{equation}
\begin{split}
\sigma_{jk}^0  =& \frac{\sum_i \delta_{ij}(x_{ij} - \bar{x}_{jk})
	     		     \delta_{ik}(x_{ik} - \bar{x}_{kj})}
		     {\sum_i \delta_{ij}\delta_{ik}},
		     \quad j = 1 \dots M \\
\bar{x}_{jk} =& \frac{\sum_i\delta_{ij} x_{ij} \delta_{ik}}
	     	     {\sum_i\delta_{ij}\delta_{ik}} \\
\bar{x}_{kj} =& \frac{\sum_i\delta_{ik} x_{ik} \delta_{ij}}
	     	     {\sum_i\delta_{ik}\delta_{ij}}
\end{split}
\end{equation}

* Re-estimate
Starting with  naively estimated $\boldsymbol{\mu}^0$  and $\boldsymbol{\Sigma}^0$,
one may guess  missing entries using conditional normal theorem. 

For a sample $\boldsymbol{x}_{i.}$, index the  observed part by $p$ and missing
part by $q$, one have
\begin{equation}
\textrm{permute}(\boldsymbol{x}_{i.}) = 
\left[\begin{array}{c}
	\boldsymbol{x}_{ip} \\
	\boldsymbol{x}_{iq}
\end{array}\right] \sim
		   \mathcal{N}
		   \left(
			\left[\begin{array}{c}
			\boldsymbol{\mu}_p^0 \\
			\boldsymbol{\mu}_q^0
		   	\end{array}\right],
			\left[\begin{array}{cc}
			\boldsymbol{\Sigma}_{pp}^0 & \boldsymbol{\Sigma}_{pq}^0 \\
			\boldsymbol{\Sigma}_{qp}^0 & \boldsymbol{\Sigma}_{qq}^0
		   	\end{array}\right]
		   \right);
\end{equation}
by conditional normal theorem, the missing part follows
<<eqn:cmn>>
\begin{equation}
\boldsymbol{x}_{iq} \sim
   \mathcal{N}\left(
	\boldsymbol{\mu}_q^0       | \boldsymbol{x}_{ip},
	\boldsymbol{\Sigma}_{qq}^0 | \boldsymbol{x}_{ip}\right)
\end{equation}
it is reasonable  to guess the missing entries in  $\boldsymbol{x}_{i.}$ by the
mean of  multivariate normal conditioned  on the  observed entries in  the same
sample, and pass on the observed entries untouched,
\begin{equation}
\begin{split}
\boldsymbol{x}_{ip}^1
	 &= \boldsymbol{x}_{ip}; \\
\boldsymbol{x}_{iq}^1
	&= \boldsymbol{\mu}_q^0 | \boldsymbol{x}_{ip}
	 = \boldsymbol{\mu}_q^0 + 
	   \boldsymbol{\Sigma}_{qp}^0
	   (\boldsymbol{\Sigma}_{pp}^0)^{-1}
	   (\boldsymbol{x}_{ip} - \boldsymbol{\mu}_p^0)
\end{split}
\end{equation}

With completed data $\boldsymbol{x}_{i.}^1$, re-estimates the mean by average
\begin{equation}
	\boldsymbol{\mu}^1 = \frac{\sum_i\boldsymbol{x}_{i.}^1}{N}
\end{equation}
It seems one could re-estimate the covariance as well,
\begin{align}
\boldsymbol{V}^1
	&= \frac{1}{N}\sum_i(\boldsymbol{V})_i^1
	 = \frac{1}{N}\sum_i
		 (\boldsymbol{x}_{i.}^1 - \boldsymbol{\mu}^1)'
		 (\boldsymbol{x}_{i.}^1 - \boldsymbol{\mu}^1); \\
\end{align}
However, $(\boldsymbol{V})_i^1$ does not account for the partial uncertainty in
$\boldsymbol{x}_{i.}^1$  expressed   by  the  covariance  of   missing  entries
$\boldsymbol{x}_{iq}$ given the observed $\boldsymbol{x}_{ip}$ ([[eqn:cmn][ref]]), that is,
\begin{align}
\begin{split}
\textrm{permute}(\boldsymbol{W})_i^1 
	&= \left[\begin{array}{cc}
	   \boldsymbol{0} & \boldsymbol{0} \\
	   \boldsymbol{0} & (\boldsymbol{W}_{qq})_i^1
   	\end{array}\right] \\
(\boldsymbol{W}_{qq})_i^1
	&= \boldsymbol{\Sigma}_{qq}^0 | \boldsymbol{x}_{ip}
	 = \boldsymbol{\Sigma}_{qq}^0 - \boldsymbol{\Sigma}_{qp}^0
	   (\boldsymbol{\Sigma}_{pp}^0)^{-1} \boldsymbol{\Sigma}_{qp}^0;
\end{split}
\end{align}
adding the uncertainty we have
\begin{equation}
\begin{split}
\boldsymbol{\Sigma}^1
	&= \frac{1}{N}\sum_i(\boldsymbol{\Sigma})_i^1
	 = \frac{1}{N}\sum_i(\boldsymbol{V})_i^1 + (\boldsymbol{W})_i^1
\end{split}
\end{equation}

* Iteration
With $\boldsymbol{\mu}^t$ and $\boldsymbol{\Sigma}^t$, one can repeat
  - complete the missing entries in $\boldsymbol{x}$ given $\boldsymbol{\mu}^t$
    and $\boldsymbol{\Sigma}^t$ (6), get $\boldsymbol{x}^{t+1}$;
  - estimate $\boldsymbol{\mu}^{t+1}$    with
    $\boldsymbol{x}^{t+1}$;
  - estimate $\boldsymbol{\Sigma}^{t+1}$ with
    $\boldsymbol{x}^{t+1}$, $\boldsymbol{\mu}^{t+1}$ and
    $\boldsymbol{\Sigma}^{t+1}$;
stop if $\boldsymbol{\Sigma}^{t+1}$ is close enough to $\boldsymbol{\Sigma}^t$,
and report $\boldsymbol{\Sigma}^t$.

* Errors
The error is the mean  difference of upper-triangle elements between covariance
estimated  from  incomplete   data  ($\boldsymbol{\Sigma}^t$),  and  covariance
estimated from the complete data ($\boldsymbol{\Sigma}^x$).
\begin{equation}
\begin{split}
\mu_j^x &= \frac{1}{N} \sum_i x_{ij},
	 \quad j = 1 \dots M \\
\sigma_{jk}^x
	&= \frac{1}{N} \sum_i (x_{ij} - \mu_j)(x_{ik} - \mu_k),
	 \quad k = 1 \dots M \\
\textrm{err}
	&= \sum_{j \le k} |\sigma_{jk}^x - \sigma_{jk}^t| \\
\end{split}
\end{equation}
